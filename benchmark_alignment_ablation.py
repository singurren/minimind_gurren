import torch
import torch.nn as nn
import pandas as pd
import time
import gc
from model.model_minimind import MiniMindConfig, MiniMindModel

# å±è”½ transformers çš„è¯¦ç»†æ—¥å¿—
import transformers
transformers.logging.set_verbosity_error()

def measure_peak_memory(name, setup_fn, train_step_fn, config, device):
    print(f"Testing {name}...", end="", flush=True)
    
    # Garbage Collection & Cache Clearing
    gc.collect()
    torch.cuda.empty_cache()
    torch.cuda.reset_peak_memory_stats()
    
    try:
        # 1. Setup Models
        models = setup_fn(config, device)
        
        # 2. Dummy Input
        batch_size = 2
        seq_len = 512
        input_ids = torch.randint(0, config.vocab_size, (batch_size, seq_len)).to(device)
        
        # 3. Training Step Simulation
        start_time = time.time()
        train_step_fn(models, input_ids)
        end_time = time.time()
        
        # 4. Measure
        peak_mem = torch.cuda.max_memory_allocated() / 1024 / 1024 # MB
        
        print(f" Done. Peak: {peak_mem:.2f} MB")
        
        # Cleanup
        del models
        del input_ids
        
        return peak_mem
    except Exception as e:
        print(f" Failed: {e}")
        return 0.0

def benchmark_alignment():
    print("ğŸš€ Starting Real-World Alignment Memory Benchmark")
    print("--------------------------------------------------")
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    if device == "cpu":
        print("âš ï¸ Warning: Running on CPU. Memory stats will be inaccurate (tracking RAM not VRAM).")

    # ä½¿ç”¨è¾ƒå°çš„é…ç½®ä»¥ç¡®ä¿åœ¨ç¬”è®°æœ¬ä¸Šèƒ½è·‘é€šå¯¹æ¯”ï¼Œé‡ç‚¹æ˜¯ç›¸å¯¹æ¯”ä¾‹
    config = MiniMindConfig(
        hidden_size=512,
        num_hidden_layers=8,
        vocab_size=6400,
        max_position_embeddings=2048
    )
    
    results = []

    # ==========================================
    # 1. SimPO (Simple Preference Optimization)
    # ==========================================
    # ç‰¹ç‚¹ï¼šNo Reference Model, No Reward/Critic Model. 
    # Loss åŒ…å« marginï¼Œç›´æ¥åœ¨ Policy ä¸Šè®¡ç®—ã€‚
    def setup_simpo(config, device):
        policy_model = MiniMindModel(config).to(device)
        optimizer = torch.optim.AdamW(policy_model.parameters(), lr=1e-5)
        return policy_model, optimizer

    def step_simpo(models, input_ids):
        model, optimizer = models
        optimizer.zero_grad()
        # Forward
        outputs, _, _ = model(input_ids)
        logits = outputs  # [B, Seq, Vocab]
        # Fake SimPO Loss (Margin based, self-contained)
        loss = logits.mean() 
        loss.backward()
        optimizer.step()

    mem_simpo = measure_peak_memory("SimPO (Policy Only)", setup_simpo, step_simpo, config, device)
    results.append({"Strategy": "SimPO", "Models Loaded": "Policy", "Peak Memory (MB)": f"{mem_simpo:.2f}"})

    # ==========================================
    # 2. DPO / GRPO (Direct Preference / Group Relative)
    # ==========================================
    # ç‰¹ç‚¹ï¼šéœ€è¦ Reference Model è®¡ç®— KL æ•£åº¦ (Ref å†»ç»“ï¼Œä¸å æ¢¯åº¦æ˜¾å­˜ï¼Œä½†å æƒé‡æ˜¾å­˜)ã€‚
    # GRPO åŒæ ·éœ€è¦ Ref Model è®¡ç®— KLï¼Œä¸”ç§»é™¤äº† Criticã€‚æ˜¾å­˜ç»“æ„ä¸ DPO ç±»ä¼¼ã€‚
    def setup_dpo(config, device):
        policy_model = MiniMindModel(config).to(device)
        ref_model = MiniMindModel(config).to(device)
        ref_model.eval().requires_grad_(False) # Ref model is frozen
        optimizer = torch.optim.AdamW(policy_model.parameters(), lr=1e-5)
        return policy_model, ref_model, optimizer

    def step_dpo(models, input_ids):
        policy, ref, optimizer = models
        optimizer.zero_grad()
        
        # Policy Forward
        policy_out, _, _ = policy(input_ids)
        
        # Ref Forward (No Grad)
        with torch.no_grad():
            ref_out, _, _ = ref(input_ids)
            
        # Fake DPO/GRPO Loss (Policy Logprobs - Ref Logprobs)
        # ç®€å•æ¨¡æ‹Ÿä¾èµ–ä¸¤ä¸ªæ¨¡å‹è¾“å‡ºçš„ Loss
        loss = (policy_out.mean() - ref_out.mean()).abs()
        loss.backward()
        optimizer.step()

    mem_dpo = measure_peak_memory("DPO/GRPO (Policy + Ref)", setup_dpo, step_dpo, config, device)
    results.append({"Strategy": "DPO / GRPO", "Models Loaded": "Policy + Ref", "Peak Memory (MB)": f"{mem_dpo:.2f}"})

    # ==========================================
    # 3. PPO (Proximal Policy Optimization - Legacy)
    # ==========================================
    # ç‰¹ç‚¹ï¼šPolicy + Reference + Critic (Value Model)ã€‚
    # Critic é€šå¸¸ä¸ Policy å¤§å°ç›¸è¿‘ã€‚è¿™æ˜¯æœ€é‡çš„ä¸€å¥—æ–¹æ¡ˆã€‚
    def setup_ppo(config, device):
        policy_model = MiniMindModel(config).to(device)
        ref_model = MiniMindModel(config).to(device)
        critic_model = MiniMindModel(config).to(device) # Critic is usually a separate model or head
        
        ref_model.eval().requires_grad_(False)
        # Critic éœ€è¦è®­ç»ƒ
        optimizer = torch.optim.AdamW(list(policy_model.parameters()) + list(critic_model.parameters()), lr=1e-5)
        return policy_model, ref_model, critic_model, optimizer

    def step_ppo(models, input_ids):
        policy, ref, critic, optimizer = models
        optimizer.zero_grad()
        
        # 1. Policy Forward
        policy_out, _, _ = policy(input_ids)
        # 2. Ref Forward
        with torch.no_grad():
            ref_out, _, _ = ref(input_ids)
        # 3. Critic Forward (Value estimation)
        value_out, _, _ = critic(input_ids)
        
        # Fake PPO Loss (Actor Loss + Critic Loss)
        loss = policy_out.mean() + value_out.mean()
        loss.backward()
        optimizer.step()

    mem_ppo = measure_peak_memory("PPO (Policy + Ref + Critic)", setup_ppo, step_ppo, config, device)
    results.append({"Strategy": "PPO (Baseline)", "Models Loaded": "Policy + Ref + Critic", "Peak Memory (MB)": f"{mem_ppo:.2f}"})

    # ==========================================
    # Conclusion
    # ==========================================
    df = pd.DataFrame(results)
    print("\nğŸ“Š Real-World Memory Measurement Results:")
    print(df.to_string(index=False))
    
    print("\nğŸ“ Engineer's Analysis:")
    if mem_simpo > 0 and mem_dpo > 0:
        saving_dpo = (mem_dpo - mem_simpo) / mem_dpo * 100
        print(f"1. [SimPO vs DPO]: SimPO é€šè¿‡ç§»é™¤ Reference Modelï¼Œæ˜¾å­˜å ç”¨é™ä½äº†çº¦ {saving_dpo:.1f}%ã€‚")
        print("   -> å…³é”®ä¼˜åŠ¿ï¼šå…è®¸åœ¨åŒæ ·ç¡¬ä»¶ä¸‹ä½¿ç”¨æ›´å¤§çš„ Batch Size æˆ–æ›´é•¿çš„ Sequenceã€‚")
    
    if mem_ppo > 0 and mem_dpo > 0:
        saving_ppo = (mem_ppo - mem_dpo) / mem_ppo * 100
        print(f"2. [GRPO vs PPO]: GRPO (åŒ DPO æ¶æ„) ç›¸æ¯”ä¼ ç»Ÿçš„ PPOï¼Œå»é™¤äº† Critic Modelï¼Œæ˜¾å­˜èŠ‚çœäº†çº¦ {saving_ppo:.1f}%ã€‚")
        print("   -> å…³é”®ä¼˜åŠ¿ï¼šä¸éœ€è¦è®­ç»ƒç‹¬ç«‹çš„ Value Networkï¼Œä¸”åˆ©ç”¨ Group é‡‡æ ·å»é™¤äº†å¯¹ Critic çš„ä¾èµ–ã€‚")

if __name__ == "__main__":
    benchmark_alignment()
