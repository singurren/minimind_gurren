import torch
import os
import argparse
import time
import numpy as np
import onnxruntime as ort
from model.model_minimind import MiniMindConfig, MiniMindForCausalLM

def export_onnx(model_path, output_path):
    print(f"ğŸš€ Starting ONNX export...")
    print(f"   Model Path: {model_path}")
    print(f"   Output Path: {output_path}")

    device = "cpu" # Exporting on CPU is usually safer/sufficient for structure
    
    # 1. Load Model
    # è¿™é‡Œçš„é…ç½®åº”è¯¥ä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼Œä¸ºæ¼”ç¤ºç›®çš„æˆ‘ä»¬ä½¿ç”¨é»˜è®¤å°å‚æ•°
    config = MiniMindConfig(
        hidden_size=512,
        num_hidden_layers=8,
        vocab_size=6400,
        max_position_embeddings=2048
    )
    model = MiniMindForCausalLM(config)
    model.eval()
    
    # å¦‚æœæœ‰çœŸå®æƒé‡ï¼Œåº”è¯¥åœ¨è¿™é‡ŒåŠ è½½
    # if os.path.exists(model_path):
    #     model.load_state_dict(torch.load(model_path, map_location=device))

    # 2. Define Dummy Input
    # Batch Size = 1, Seq Len = 64
    dummy_input = torch.randint(0, config.vocab_size, (1, 64)).to(device)

    # 3. Export to ONNX
    # å·¥ä¸šç•Œéƒ¨ç½²é€šå¸¸éœ€è¦æ”¯æŒåŠ¨æ€ Batch å’ŒåŠ¨æ€ Sequence Length
    input_names = ["input_ids"]
    output_names = ["logits"]
    
    dynamic_axes = {
        "input_ids": {0: "batch_size", 1: "seq_len"},
        "logits": {0: "batch_size", 1: "seq_len"}
    }

    torch.onnx.export(
        model,
        (dummy_input,),
        output_path,
        input_names=input_names,
        output_names=output_names,
        dynamic_axes=dynamic_axes,
        opset_version=14, # è¾ƒé«˜çš„ opset æ”¯æŒæ›´å¤šç®—å­
        do_constant_folding=True # ä¼˜åŒ–å›¾ç»“æ„
    )
    print(f"âœ… Model exported to {output_path}")

    # 4. Verify Export
    verify_onnx(model, output_path)

def verify_onnx(torch_model, onnx_path):
    print("\nğŸ” Verifying ONNX model correctness...")
    
    # Create a test input different from dummy input
    test_input = torch.randint(0, 6400, (2, 128))
    
    # 1. PyTorch Output
    with torch.no_grad():
        torch_out = torch_model(test_input).logits.numpy()
        
    # 2. ONNX Runtime Output
    ort_session = ort.InferenceSession(onnx_path)
    ort_inputs = {ort_session.get_inputs()[0].name: test_input.numpy()}
    ort_out = ort_session.run(None, ort_inputs)[0]
    
    # 3. Compare
    # å…è®¸ä¸€å®šçš„ç²¾åº¦è¯¯å·® (fp32é€šå¸¸åœ¨1e-5çº§åˆ«)
    diff = np.max(np.abs(torch_out - ort_out))
    print(f"   Max Difference: {diff:.2e}")
    
    if diff < 1e-4: # æ”¾å®½ä¸€ç‚¹ç‚¹ï¼Œè€ƒè™‘åˆ°ä¸åŒåç«¯çš„æµ®ç‚¹å·®å¼‚
        print("âœ… Export Verified! The ONNX model matches PyTorch outputs.")
        print("\nğŸ’¡ Engineer's Note:")
        print("   ONNX (Open Neural Network Exchange) æ˜¯é€šå¾€é«˜æ€§èƒ½æ¨ç†å¼•æ“ (å¦‚ TensorRT) çš„å…³é”®æ¡¥æ¢ã€‚")
        print("   é€šè¿‡å°†åŠ¨æ€å›¾ (PyTorch) è½¬æ¢ä¸ºé™æ€è®¡ç®—å›¾ (ONNX)ï¼Œæˆ‘ä»¬å¯ä»¥è¿›è¡Œï¼š")
        print("   1. ç®—å­èåˆ (Operator Fusion): å‡å°‘ GPU Kernel å¯åŠ¨å¼€é”€ã€‚")
        print("   2. ç²¾åº¦é‡åŒ– (Quantization): æ–¹ä¾¿åœ°è½¬æ¢ä¸º FP16/INT8ã€‚")
        print("   3. è·¨å¹³å°éƒ¨ç½²: ä¸€æ¬¡å¯¼å‡ºï¼Œåˆ°å¤„è¿è¡Œ (Triton Server, Edge Devices)ã€‚")
    else:
        print("âŒ Verification Failed! Difference is too large.")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Export MiniMind to ONNX")
    parser.add_argument("--model_path", type=str, default=None, help="Path to PyTorch model weights")
    parser.add_argument("--output_path", type=str, default="minimind.onnx", help="Output ONNX file path")
    args = parser.parse_args()

    export_onnx(args.model_path, args.output_path)
