import torch
import time
from model.model_minimind import MiniMindConfig, MiniMindModel
import pandas as pd

def benchmark_attention():
    print("ğŸš€ Starting Architecture Ablation: GQA vs MHA")
    print("--------------------------------------------------")
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    if device == "cpu":
        print("âš ï¸ Warning: Running on CPU, memory metrics might not reflect GPU usage accurately.")

    # ä¿æŒæ¨¡å‹å…¶ä»–å‚æ•°ä¸€è‡´ï¼Œåªä¿®æ”¹ attention heads
    base_config = {
        "hidden_size": 512,
        "num_hidden_layers": 8,
        "vocab_size": 6400,
        "max_position_embeddings": 4096,
        "num_attention_heads": 8,
    }

    configs = {
        "Model A (MHA)": MiniMindConfig(**base_config, num_key_value_heads=8), # MHA: KV heads = Query heads
        "Model B (GQA)": MiniMindConfig(**base_config, num_key_value_heads=2)  # GQA: KV heads = 1/4 Query heads
    }

    results = []

    # Dummy Input
    batch_size = 8
    seq_len = 2048 # é•¿åºåˆ—æ›´èƒ½ä½“ç° KV Cache å·®å¼‚
    dummy_input = torch.randint(0, 6400, (batch_size, seq_len)).to(device)

    for name, config in configs.items():
        print(f"Testing {name}...")
        model = MiniMindModel(config).to(device)
        model.eval()

        # Warmup
        with torch.no_grad():
            for _ in range(5):
                _ = model(dummy_input)
        
        torch.cuda.reset_peak_memory_stats()
        start_time = time.time()
        
        # Testing
        with torch.no_grad():
            for _ in range(50):
                # æ¨¡æ‹Ÿæ¨ç†ç”Ÿæˆè¿‡ç¨‹ä¸­çš„ KV Cache å¢é•¿
                # è¿™é‡Œç®€å•åšä¸€æ¬¡ forward passï¼Œä¸»è¦çœ‹ peak memory
                # åœ¨çœŸå®æ¨ç†ä¸­ï¼ŒKV Cache æ˜¯é€æ­¥å¢é•¿çš„ï¼Œè¿™é‡Œ max memory ä¼šåæ˜ å‡ºå­˜å‚¨ cache æ‰€éœ€çš„æ˜¾å­˜
                _ = model(dummy_input, use_cache=True)
        
        end_time = time.time()
        
        max_mem = torch.cuda.max_memory_allocated() / 1024 / 1024 # MB
        avg_latency = (end_time - start_time) / 50 * 1000 # ms
        
        results.append({
            "Model": name,
            "KV Heads": config.num_key_value_heads,
            "Peak Memory (MB)": f"{max_mem:.2f}",
            "Latency (ms)": f"{avg_latency:.2f}"
        })
        
        del model
        torch.cuda.empty_cache()

    df = pd.DataFrame(results)
    print("\nğŸ“Š Ablation Results:")
    print(df.to_string(index=False))
    
    # ç»“è®ºç”Ÿæˆ
    print("\nğŸ“ Conclusion:")
    mha_mem = float(results[0]["Peak Memory (MB)"])
    gqa_mem = float(results[1]["Peak Memory (MB)"])
    saving = (mha_mem - gqa_mem) / mha_mem * 100
    print(f"GQA ç›¸æ¯” MHA èŠ‚çœäº† {saving:.1f}% çš„æ˜¾å­˜ã€‚")
    print("åœ¨é•¿ä¸Šä¸‹æ–‡ (2048+) æ¨ç†åœºæ™¯ä¸‹ï¼ŒGQA æ˜¾è‘—é™ä½äº† KV Cache çš„æ˜¾å­˜å ç”¨ï¼Œä»è€Œå…è®¸æ›´å¤§çš„ Batch Size æˆ–æ›´é•¿çš„ Sequence Lengthã€‚")

if __name__ == "__main__":
    benchmark_attention()
