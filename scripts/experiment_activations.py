import torch
import torch.nn as nn
import time
from transformers.activations import ACT2FN

# ğŸ“˜ğŸ“˜ğŸ“˜ å®éªŒè¯´æ˜ ğŸ“˜ğŸ“˜ğŸ“˜
# è¿™ä¸ªè„šæœ¬ç”¨äºå¯¹æ¯”æ ‡å‡†MLPï¼ˆNon-Gatedï¼‰å’ŒSwiGLUï¼ˆGatedï¼‰ç»“æ„çš„å‚æ•°é‡å’Œæ¨ç†é€Ÿåº¦ã€‚
# æˆ‘åœ¨é€‰æ‹©FeedForwardå±‚ç»“æ„æ—¶ï¼Œçº ç»“äºä½¿ç”¨ç»å…¸çš„GELU MLPè¿˜æ˜¯SwiGLUã€‚
# è™½ç„¶SwiGLUå‚æ•°é‡å¤šäº†1/3ï¼ˆ3ä¸ªçº¿æ€§å±‚ vs 2ä¸ªï¼‰ï¼Œä½†æ–‡çŒ®è¡¨æ˜å…¶æ”¶æ•›æ€§èƒ½æ›´å¥½ã€‚
# æ­¤è„šæœ¬ç”¨äºé‡åŒ–äºŒè€…çš„æ€§èƒ½å¼€é”€å·®è·ï¼Œä»¥å†³å®šæ˜¯å¦å€¼å¾—å¼•å…¥é¢å¤–çš„å‚æ•°é‡ã€‚

class StandardMLP(nn.Module):
    def __init__(self, hidden_size, intermediate_size):
        super().__init__()
        # Standard: Up -> Act -> Down
        self.up_proj = nn.Linear(hidden_size, intermediate_size, bias=False)
        self.down_proj = nn.Linear(intermediate_size, hidden_size, bias=False)
        self.act_fn = nn.GELU()

    def forward(self, x):
        return self.down_proj(self.act_fn(self.up_proj(x)))

class SwiGLUMLP(nn.Module):
    def __init__(self, hidden_size, intermediate_size):
        super().__init__()
        # SwiGLU: (Gate -> Act) * Up -> Down
        self.gate_proj = nn.Linear(hidden_size, intermediate_size, bias=False)
        self.up_proj = nn.Linear(hidden_size, intermediate_size, bias=False)
        self.down_proj = nn.Linear(intermediate_size, hidden_size, bias=False)
        self.act_fn = nn.SiLU()

    def forward(self, x):
        return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))

def count_parameters(model):
    return sum(p.numel() for p in model.parameters())

def benchmark(model, x, n_iters=100):
    model.eval()
    # é¢„çƒ­
    for _ in range(10):
        with torch.no_grad():
            _ = model(x)
    
    torch.cuda.synchronize() if torch.cuda.is_available() else None
    start = time.time()
    for _ in range(n_iters):
        with torch.no_grad():
            _ = model(x)
    torch.cuda.synchronize() if torch.cuda.is_available() else None
    end = time.time()
    return (end - start) / n_iters

def main():
    hidden_size = 512
    # ä¸ºäº†ä¿æŒå‚æ•°é‡è¿‘ä¼¼å¯æ¯”ï¼ŒStandardMLPçš„intermediate_sizeé€šå¸¸æ›´å¤§ï¼Œ
    # ä½†SwiGLUé€šå¸¸è®¾ä¸º 8/3 * hidden_sizeã€‚
    # è¿™é‡Œæˆ‘ä»¬æ§åˆ¶intermediate_sizeç›¸åŒï¼Œç›´æ¥çœ‹å‚æ•°é‡å¢åŠ çš„æ¯”ä¾‹ã€‚
    intermediate_size = int(hidden_size * 4) 
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    
    std_mlp = StandardMLP(hidden_size, intermediate_size).to(device)
    swiglu_mlp = SwiGLUMLP(hidden_size, intermediate_size).to(device)
    
    print(f"=== MLP ç»“æ„å¯¹æ¯” (Hidden={hidden_size}, Inter={intermediate_size}) ===")
    
    # 1. å‚æ•°é‡å¯¹æ¯”
    p_std = count_parameters(std_mlp)
    p_swiglu = count_parameters(swiglu_mlp)
    print(f"Standard MLP å‚æ•°é‡: {p_std}")
    print(f"SwiGLU MLP å‚æ•°é‡:   {p_swiglu} (+{((p_swiglu - p_std)/p_std)*100:.2f}%)")
    
    # 2. é€Ÿåº¦å¯¹æ¯”
    batch_size = 32
    seq_len = 128
    x = torch.randn(batch_size, seq_len, hidden_size).to(device)
    
    t_std = benchmark(std_mlp, x)
    t_swiglu = benchmark(swiglu_mlp, x)
    
    print(f"\nå¹³å‡æ¨ç†æ—¶é—´ (batch={batch_size}, seq={seq_len}):")
    print(f"Standard MLP: {t_std*1000:.4f} ms")
    print(f"SwiGLU MLP:   {t_swiglu*1000:.4f} ms (+{((t_swiglu - t_std)/t_std)*100:.2f}%)")
    
    print("\nç»“è®ºï¼š")
    print("SwiGLUå¼•å…¥äº†é¢å¤–çš„GateæŠ•å½±å±‚ï¼Œå¯¼è‡´å‚æ•°é‡å’Œè®¡ç®—é‡å‡å¢åŠ çº¦50%ï¼ˆåœ¨ç›¸åŒintermediate_sizeä¸‹ï¼‰ã€‚")
    print("ä½†åœ¨å®é™…åº”ç”¨ä¸­ï¼Œæˆ‘ä»¬é€šå¸¸ä¼šè°ƒæ•´intermediate_sizeï¼ˆä¾‹å¦‚ä»4hé™åˆ°8/3hï¼‰æ¥å¹³è¡¡å‚æ•°é‡ã€‚")
    print("æœ€ç»ˆå†³å®šï¼šé‡‡ç”¨SwiGLUï¼Œå› ä¸ºå…¶å¸¦æ¥çš„PPLæ”¶ç›Šé€šå¸¸ä¼˜äºå•çº¯å¢åŠ æ·±åº¦ã€‚")

if __name__ == "__main__":
    main()
