import re
import os
import hashlib
import json
import argparse
from typing import List, Set

# å°è¯•å¯¼å…¥ datasketchï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨ Mock
try:
    from datasketch import MinHash, MinHashLSH
    HAS_DATASKETCH = True
except ImportError:
    HAS_DATASKETCH = False
    print("âš ï¸ Warning: 'datasketch' library not found. Using simple hash-based deduplication fallback.")

class DataProcessor:
    """
    å·¥ä¸šçº§æ•°æ®å¤„ç† Pipeline
    åŠŸèƒ½ï¼š
    1. MinHash LSH æ¨¡ç³Šå»é‡ï¼šè§£å†³å¤§è§„æ¨¡è¯­æ–™ä¸­çš„è¿‘ä¹‰é‡å¤é—®é¢˜ã€‚
    2. å¯å‘å¼è´¨é‡è¿‡æ»¤ï¼šå»é™¤ä½è´¨é‡ã€å™ªå£°å¤§çš„æ–‡æœ¬ã€‚
    """
    def __init__(self, threshold=0.8, num_perm=128):
        self.threshold = threshold
        self.num_perm = num_perm
        if HAS_DATASKETCH:
            self.lsh = MinHashLSH(threshold=self.threshold, num_perm=self.num_perm)
        self.seen_hashes = set()
        self.total_processed = 0
        self.total_deduplicated = 0
        self.total_filtered = 0

    def _get_minhash(self, text: str):
        """ç”Ÿæˆæ–‡æœ¬çš„ MinHash æŒ‡çº¹"""
        m = MinHash(num_perm=self.num_perm)
        # N-gram shingling
        tokens = [text[i:i+3] for i in range(len(text)-2)]
        for t in tokens:
            m.update(t.encode('utf8'))
        return m

    def is_duplicate(self, text: str, doc_id: str) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦ä¸ºé‡å¤æ–‡æœ¬
        å¦‚æœ datasketch å¯ç”¨ï¼Œä½¿ç”¨ LSHï¼›å¦åˆ™ä½¿ç”¨ç²¾å‡† Hash åŒ¹é…ã€‚
        """
        if HAS_DATASKETCH:
            minhash = self._get_minhash(text)
            results = self.lsh.query(minhash)
            if results:
                return True
            self.lsh.insert(doc_id, minhash)
            return False
        else:
            # Fallback: Simple MD5 exact match
            h = hashlib.md5(text.encode('utf-8')).hexdigest()
            if h in self.seen_hashes:
                return True
            self.seen_hashes.add(h)
            return False

    def quality_filter(self, text: str) -> bool:
        """
        åŸºäºå¯å‘å¼è§„åˆ™çš„è´¨é‡è¿‡æ»¤
        Returns: True if passed (keep), False if filtered (drop)
        """
        if len(text) < 20: # è¿‡æ»¤å¤ªçŸ­çš„æ–‡æœ¬
            return False
        
        # è¿‡æ»¤ä»£ç å¯†åº¦è¿‡é«˜çš„æ–‡æœ¬ (å¦‚æœç›®æ ‡æ˜¯é€šç”¨è‡ªç„¶è¯­è¨€)
        if text.count('{') + text.count('}') > len(text) * 0.1:
            return False

        # è¿‡æ»¤æ ‡ç‚¹ç¬¦å·è¿‡å°‘çš„æ–‡æœ¬ (å¯èƒ½æ˜¯å™ªéŸ³)
        import string
        punc_count = sum([1 for char in text if char in string.punctuation])
        if punc_count / len(text) < 0.01:
            return False

        return True

    def process_file(self, input_path: str, output_path: str):
        print(f"ğŸ”„ Processing {input_path}...")
        
        # æ¨¡æ‹Ÿè¯»å–å’Œå¤„ç†
        # å®é™…åœºæ™¯ä¸­åº”æµå¼è¯»å– (yield) ä»¥å¤„ç†å¤§æ–‡ä»¶
        if not os.path.exists(input_path):
             # åˆ›å»ºä¸€ä¸ª Dummy æ–‡ä»¶ç”¨äºæ¼”ç¤º
             print("Create dummy file for demo...")
             with open(input_path, 'w', encoding='utf-8') as f:
                 f.write(json.dumps({"text": "DeepSeek æ˜¯ä¸€ä¸ªå¾ˆæ£’çš„æ¨¡å‹ã€‚"}) + "\n")
                 f.write(json.dumps({"text": "DeepSeek æ˜¯ä¸€ä¸ªå¾ˆæ£’çš„æ¨¡å‹ã€‚"}) + "\n") # Duplicate
                 f.write(json.dumps({"text": "åƒåœ¾æ•°æ®"}) + "\n") # Low quality

        with open(input_path, 'r', encoding='utf-8') as fin, \
             open(output_path, 'w', encoding='utf-8') as fout:
            
            for i, line in enumerate(fin):
                try:
                    data = json.loads(line)
                    text = data.get("text", "")
                    doc_id = f"doc_{i}"

                    # 1. Quality Filter
                    if not self.quality_filter(text):
                        self.total_filtered += 1
                        continue

                    # 2. Deduplication
                    if self.is_duplicate(text, doc_id):
                        self.total_deduplicated += 1
                        continue

                    # Write clean data
                    fout.write(line)
                    self.total_processed += 1

                except json.JSONDecodeError:
                    continue
        
        print(f"âœ… Processing Done.")
        print(f"   Saved: {self.total_processed}")
        print(f"   Filtered (Quality): {self.total_filtered}")
        print(f"   Filtered (Duplicate): {self.total_deduplicated}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Data Processing Pipeline")
    parser.add_argument("--input", type=str, default="dataset/raw_data.jsonl")
    parser.add_argument("--output", type=str, default="dataset/clean_data.jsonl")
    args = parser.parse_args()
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs("dataset", exist_ok=True)
    
    processor = DataProcessor()
    processor.process_file(args.input, args.output)
